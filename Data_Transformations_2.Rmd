---
title: "Data Transformation"
author: "Andrew Zinkan"
date: "8/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,  results= "hide", message=FALSE, warning=FALSE}
# Load initial packages
library(tidyverse)
library(ggplot2)
library(dplyr)
library(vtable)
library(fixest)
library(tidyr)
library(lubridate)
```
## Data Transformation for Score Card Data
$Note:$This file needs to be run before the EDA_3.Rmd can executed. 

#### _Loading the initial data_
```{r, results= "hide", message=FALSE, warning=FALSE}
# Running the raw data
# Striping the Nulls to NA Values
DataDictRaw <-  read.csv("Data_Exploration/CollegeScorecardDataDictionary.csv", na.strings = c("","NA","NULL",NULL))
IDNameLinkRaw <- read.csv("Data_Exploration/id_name_link.csv", na.strings = c("","NA","NULL",NULL))
ScoreCardDataRaw <- read.csv("Data_Exploration/Most_Recent_Cohorts_Scorecard_Elements.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToFinishRaw <- read.csv("Data_Exploration/trends_up_to_finish.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_1_Raw <- read.csv("Data_Exploration/trends_up_to_inter_1.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_2_Raw <- read.csv("Data_Exploration/trends_up_to_inter_2.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_3_Raw <- read.csv("Data_Exploration/trends_up_to_inter_3.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_4_Raw <- read.csv("Data_Exploration/trends_up_to_inter_4.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_5_Raw <- read.csv("Data_Exploration/trends_up_to_inter_5.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_6_Raw <- read.csv("Data_Exploration/trends_up_to_inter_6.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUmRaw <- read.csv("Data_Exploration/trends_up_to_UM.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUphoenixRaw <- read.csv("Data_Exploration/trends_up_to_UPhoenix.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUtRaw <- read.csv("Data_Exploration/trends_up_to_UT.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUtmbRaw <- read.csv("Data_Exploration/trends_up_to_UTMB.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToYorktowneRaw <- read.csv("Data_Exploration/trends_up_to_Yorktowne.csv", na.strings = c("","NA","NULL",NULL))
```

#### _Drop opeid's that are not unique observations_

I'm doing this to avoid any `many to one` relationships when joining **OPEID**
```{r}
Unique_opeid <- ScoreCardDataRaw %>% 
  select(OPEID) %>% 
  group_by(OPEID) %>% 
  summarise(count_rec = n())%>% 
  filter(count_rec == 1) %>% 
  arrange(., desc(count_rec))


# create filter for unique ids
Unique_opeid_filter <- Unique_opeid$OPEID

# filter raw set by the opeids that have unique names
ScoreCardDataStage <- filter(ScoreCardDataRaw, OPEID %in% Unique_opeid_filter)

```


#### _Remove "PrivacySuppressed" earnings data_

Some records provided in this data set have earnings data either NA or marked as "PrivacySuppressed". Because this variable is our measurement for college earnings which is a central part of our research question, we need to only work with known values for this. 

Null values in this field don't shed any light on how different earnings impact search rate if earning are unknown. 
```{r}
# Remove "PrivacySuppressed" data 
NotSuppressed <- ScoreCardDataStage %>% 
  select(OPEID, md_earn_wne_p10.REPORTED.EARNINGS) %>% 
  filter(md_earn_wne_p10.REPORTED.EARNINGS != c("PrivacySuppressed"))

# create filter for unique ids
NotSuppressed_filter <- NotSuppressed$OPEID

# Remove Suppressed Earnings Data
ScoreCardDataStage <- filter(ScoreCardDataStage, OPEID %in% NotSuppressed_filter)
```

#### _Drop NULL values in earnings data_
I want to drop null values for the same reason we want to drop the suppressed data. Null value here don't provide any use to this research question.  

```{r}
NotNull <- ScoreCardDataStage %>% 
  select(OPEID, md_earn_wne_p10.REPORTED.EARNINGS) %>% 
  filter(!is.na(md_earn_wne_p10.REPORTED.EARNINGS))

# create filter for unique ids
NotNull_Filter <- NotNull$OPEID

# Remove Suppressed Earnings Data
ScoreCardDataStage <- filter(ScoreCardDataStage, OPEID %in% NotNull_Filter)
```

#### _Renaming_

Doing a minimal amount of renaming and mutations here but it makes these variables esier to work with later on. 
```{r}
ScoreCardDataStage <- ScoreCardDataStage %>% 
  mutate(md_earn_wne_p10.REPORTED.EARNINGS = as.numeric(ScoreCardDataStage$md_earn_wne_p10.REPORTED.EARNINGS))

ScoreCardDataStage <- ScoreCardDataStage %>%
  rename(opeid = OPEID,  
         unitid = UNITID, 
         md_reported_ern_10yr = md_earn_wne_p10.REPORTED.EARNINGS)

```

## Data Transformation For School Link Data

#### _Drop non unique opeid's_
Here I'm identify opeid's with more than one unique school name associated as we dont want to have a `many to one` relationship with opeid when we make a mastered data set. We don't want a `many to one` or `many to many` as this can cause records duplication. 

```{r}
Unique_schools <- IDNameLinkRaw %>% 
  select(opeid, schname) %>% 
  group_by(opeid) %>% 
  summarise(count_unique = n_distinct(schname))%>% 
  arrange(., desc(count_unique)) %>% 
  filter(count_unique == 1)

# create filter for unique ids
school_filter <- Unique_schools$opeid

# filter raw set by the opeids that have unique names
IDNameLinkStage <- filter(IDNameLinkRaw, opeid %in% school_filter)
```

#### _Drop non unique Universities Names_
Here I'm finding all the school names that are tied to more than one **opeid**. This is another instance where a `many to many` or `many to one` could cause duplication issues down stream. 

```{r}
# Identify names that have more than one opeid
unique_ids <- IDNameLinkRaw %>% 
  select(opeid, schname) %>% 
  group_by(schname) %>% 
  summarise(count_unique = n_distinct(opeid))%>% 
  arrange(., desc(count_unique)) %>% 
  filter(count_unique < 2)

# create filter for unique ids
school_filter <- unique_ids$schname

# apply another filter to staged set by filtering the opeids that have unique names
IDNameLinkStage <- filter(IDNameLinkStage, schname %in% school_filter)
```

#### _Filter bachelor degree colleges_
Selecting Bachelors Primary Schools & Bachelors Only Schools.
Here I'm using **PREDDEG >= 3** as schools that offer *primarily* bachelors degrees would consist of both primary degree categories 3 and 4. 

- category 3 -> primarily offer bachelors degrees
- category 4 -> exclusively over bachelor degrees

```{r}
# Selecting Bachelors Primary Schools & Bachelors Only Schools
# Bachelors Only Schools are by default Bachelors Primary
Bachelors_primary <- ScoreCardDataStage %>% 
  select(opeid, PREDDEG) %>% 
  filter(PREDDEG >= 3)

# create filter for unique ids
bach_filter <- Bachelors_primary$opeid

# Remove non bachelors from our master id list
IDNameLinkStage <- filter(IDNameLinkStage, opeid %in% bach_filter)
```

#### _Drop "PrivacySuppressed" earnings_
Some records provided in this data set have earnings data either NA or marked as "PrivacySuppressed". Because this variable is our measurement for college earnings which is a central part of our research question, we need to only work with known values for this. 
```{r}
# Remove "PrivacySuppressed" data 
NotSuppressed <- ScoreCardDataStage %>% 
  select(opeid, md_reported_ern_10yr) %>% 
  filter(md_reported_ern_10yr != c("PrivacySuppressed"))

# create filter for unique ids
NotSuppressed_filter <- NotSuppressed$opeid

# Remove Suppressed Earnings Data
IDNameLinkStage <- filter(IDNameLinkStage, opeid %in% NotSuppressed_filter)
```

#### _Drop Null earnings_
Drop null values for the same reason we want to drop the suppressed data 

```{r}
NotNull <- ScoreCardDataStage %>% 
  select(opeid, md_reported_ern_10yr) %>% 
  filter(!is.na(md_reported_ern_10yr))

# create filter for unique ids
NotNull_Filter <- NotNull$opeid

# Remove Suppressed Earnings Data
IDNameLinkStage <- filter(IDNameLinkStage, opeid %in% NotNull_Filter)
```

## Data Transformation Trend data


#### _Union All Trend Data_
union all the Trends Data as one data frame to make it easier to work with.
```{r, results= "hide", message=FALSE, warning=FALSE}
# Union the Trends Data
TrendsAppendedDf <- rbind(TrendsUpToInter_1_Raw, TrendsUpToInter_2_Raw, TrendsUpToInter_3_Raw, TrendsUpToInter_4_Raw, TrendsUpToInter_5_Raw, TrendsUpToInter_6_Raw, TrendsUpToUmRaw, TrendsUpToUphoenixRaw, TrendsUpToUtmbRaw, TrendsUpToUtmbRaw, TrendsUpToYorktowneRaw, TrendsUpToFinishRaw )
```

#### _Drop Duplicates_
Since this is API generated trends data that has breaks when to many requests are generated its very possible that when we resend a new request old records are loaded again, hence creating duplicates.

Regardless if that's the case we don't want to have any duplicate records for this panel data set, and because the *schid* field has no known or understandable meaning im choosing to remove that field then only select distinct values across the full data frame. 
```{r}
# Select remove the schid field (non unique) and no relationship
Trends <- TrendsAppendedDf %>% 
  select( -c("schid"))

Trends <- Trends %>% 
  distinct()
```

#### _Strip Dates_
Dates come in a text columns provided as a range of dates (ex: 2022-01-01 - 2022-01-07).In order to get a date value form this value im stripping the string partitioned by " - " and taking the first value as the start of the week that the trend data occurred. This date is converted to a date.time() value for easier use down stream. 

I then added a binary variable to indicate if the trend data pertains to before or after the College Score Card launch date (2015-09-01).

```{r, message=FALSE, warning=FALSE}
# separate dates
Trends <- separate(Trends, col=monthorweek, into=c('start_of_week', 'end_of_week'), sep=' - ')

# Convert to date type
Trends <- Trends %>% 
  mutate(start_of_week = as.Date(start_of_week, format="%Y-%m-%d")) %>% 
  mutate(end_of_week = as.Date(end_of_week, format="%Y-%m-%d"))

# Before and after scorecard indicator
Trends <- Trends %>% 
  mutate(after_score_card = if_else(start_of_week >= as.Date("2015-09-01", format="%Y-%m-%d"), TRUE, FALSE))

```

#### _No Null Dates_
As this question is related to the function of time we need to deal with the records that are not associated with a time frame.
It should be noted that these records also lack any index information. Likely this was complied from source incorrectly or reflects API queries where data was not available from the host.

Regardless we need to remove these records as they provide no value to our analysis. 

```{r}
Trends <- Trends %>% 
  filter(!is.na(start_of_week))
```

#### _Make Indices reasonably comparable_

Here we are making the search index's reasonably comparable across terms. 
To make the indices reasonably comparable we need to standardizing them for each term, by subtracting the mean and divide by the standard deviation. 

```{r, message=FALSE, warning=FALSE}
# Key Level Mean
key_level_idx <- Trends %>% 
  select(schname,keyword,index, ) %>% 
  group_by(schname,keyword) %>% 
  summarise(mean_index = mean(index), 
            std_index = sd(index))

# Join raw and idk level
Trends <- Trends %>% 
  inner_join(key_level_idx, by = c("schname", "keyword")) %>% 
  mutate(standard_idx = (index-mean_index)/std_index) %>% 
  select(schname, keyword, start_of_week,  standard_idx, after_score_card)

```

#### _Add Month info_
Transforming the data value to a monthly expression in case i want to do analysis at the level. 
```{r}
# add month
Trends <- Trends %>% 
  mutate(start_of_month = format(as.Date(start_of_week), "%Y-%m-1"))
```

#### _Aggregate the Trends Data_
Here im aggregating the results for Google trends data, selecting to group by school & week. Im selecting this level as i want to resolve the number of key words tracked per school. come schools track 1 work where others track 7. 

In order to farther standardized the data set and make metrics comparable across shool i just just want the average seach per school per week. This allows me to keep college name so that i can tie colleges together and join the associated measures.Im choseing the keep this data at the weekly level as it retains more granularity and more data points for us to workwith in the analysis. 

Data can always be aggregated at a higher level down stream if need be. 
```{r}
Trends <- Trends %>% 
  group_by(schname, after_score_card, start_of_week, start_of_month) %>%
  summarise(md_wk_std_idx = median(standard_idx)) %>%
  arrange(., start_of_week )
```
## Master Data


#### _School ID & Metrics_
Using inner join to ensure that we have a matching data set in terms of unitid and opeid. Inner join helps us filter out anything that doesn't match between the two data sets.
```{r}
school_master <- IDNameLinkStage %>% 
  inner_join(ScoreCardDataStage, by = c("opeid", "unitid"))
```

#### _Google Trends & School ID_
Using inner join to ensure that we have a only matching school names across our school id info and our google trends info. 
Joining on the **schname** across data sets. Both of these staged table are already at the unique **schname** level. 
```{r}
google_trends_master <- IDNameLinkStage %>% 
  inner_join(Trends, by = c("schname"))
```


## CSV Out Put
Producing an output file as a csv that can be stored as prep file.

By doing this I can simply read in the prep file down stream and not be required to reproduce all data proccessing steps. (saves time and computation while doing analysis) 
```{r}
write.csv(school_master, "Generated_data\\school_master.csv", row.names=FALSE)
```

```{r}
write.csv(google_trends_master, "Generated_data\\google_trends_master.csv", row.names=FALSE)
```



