---
title: "EDA"
author: "Andrew Zinkan"
date: "8/3/2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, results= "hide", message=FALSE, warning=FALSE}
# Load initial packages
# install.packages("rlist")
library(tidyverse)
library(ggplot2)
library(dplyr)
library(vtable)
library(fixest)
library(Hmisc)
# library(timetk)
# library(DMwR)
library(tidyr)
library(rlist)
library(lubridate)
library(summarytools)
```
###  Restated Question:

The College Scorecard was released at the start of September 2015. Among colleges that predominantly grant bachelor’s degrees, did the release of the Scorecard shift student interest to high-earnings colleges relative to low-earnings ones (as proxied by Google searches for keywords associated with those colleges)?

#### _Load Data Transformation Files_
These Files are produced in the Data Transformation Phase -> Data_Transformations_2.Rmd and then joined in this file to produce our mastered data set for analysis.
```{r, results= "hide", message=FALSE, warning=FALSE}
google_trends_master <-  read.csv("Generated_data/google_trends_master.csv", na.strings = c("","NA","NULL",NULL))
school_master <- read.csv("Generated_data/school_master.csv", na.strings = c("","NA","NULL",NULL))

```

```{r, echo = FALSE ,results= "hide", message=FALSE, warning=FALSE}
google_trends_master <- google_trends_master %>% 
  mutate(start_of_week = as.Date(start_of_week, format="%Y-%m-%d")) %>% 
  mutate(start_of_month = as.Date(start_of_month, format="%Y-%m-%d")) %>% 
  mutate(month_num = month(start_of_month))

# cat(paste(colnames(school_master), collapse = "\n "))
  
```
```{r, echo= FALSE}
# Make the final data set and provide reasoning
df <- google_trends_master %>% 
  inner_join(school_master, by = c("opeid", "unitid"))

df <- df %>% 
  rename(schname = schname.x) %>% 
  select(-c("schname.y", "unitid", "opeid", "opeid6", "INSTNM", "INSTURL", 
            "NPCURL"))
  
# str(df)


```

#### _Conceptual DAG_

The diagram below represents some of the high level variable interactions that pertain to our research question. 
Notice that we have a lot of factors that play into the *Google Search Terms* but not many that directly impact *College Score Card*. 

We also have a lot of *un-measurable* high level variables that are related to search volumes, such as current events, news coverage, prestige or brand awareness, & culture. 

All of these variables can play into the Google Search traffic but they *dont* impact the score card release date.  

The only thing that needs to be controlled in this case, is time or seasonality as anything that changes over time is a potential source of endogeneity for a time related event.  

*** 
$Note:$
The Scorecard policy is implemented at a particular time, and looking at how the Scorecard shifted interest really means looking at whether interest shifted at that particular time. This means that anything that changes over time is a potential source of endogeneity, even if it isn't inherently related to the Scorecard

![Dagity](dagity.jpg)


# Variable Selection
There are a lot of variables in this data set available for use, however many of which do not have an intuitive or direct connection to Google search volumes or the release of the College Score Card. Before I go into EDA and modeling, I want to reduce our data to a more concise and targeted variable set. 

#### _Removal of PCIP Variables_
The *PCIC* or "Percentage of degrees awarded in a {specific field}" variables are  high column count but low in explanatory value, at least for our research question. Intuitively, I can't reason how or why the percentage of degrees awarded for a particular field would be at all related to Google search traffic for a University, nor would it impact when the score card was released.

I'm also making the assumption that College score card does _not_ cater towards specific programs, thus it does not disproportional generate more searches for a specific type of program. 

Just for good measure I'm going to run a very simply linear model to see if there are any variables worth looking into more. 
From the results (below) I conclude that these variables do not have individual or cumulative explanatory power over our dependent variable (search volume).

$Note:$ Removing these variables from analysis. 
```{r, echo= FALSE}
# paste(colnames(df), collapse = " + ")

m1 <- feols(md_wk_std_idx ~ PCIP03 + PCIP04 + PCIP05 + PCIP09 + PCIP10 + PCIP11 + PCIP12 + PCIP13 + PCIP14 + PCIP15 + PCIP16 + PCIP19 + PCIP22 + PCIP23 + PCIP24 + PCIP25 + PCIP26 + PCIP27 + PCIP29 + PCIP30 + PCIP31 + PCIP38 + PCIP39 + PCIP40 + PCIP41 + PCIP42 + PCIP43 + PCIP44 + PCIP45 + PCIP46 + PCIP47 + PCIP48 + PCIP49 + PCIP50 + PCIP51 + PCIP52 ,data = df)
etable(m1)
```
```{r, echo = FALSE}

# paste(dQuote(colnames(df)), collapse = ", ")

#removing variables with alot of na to cut down analysis
df <- df %>% 
  select(-c("PCIP01", "PCIP03", "PCIP04", "PCIP05", "PCIP09", "PCIP10", "PCIP11", "PCIP12", "PCIP13", "PCIP14", "PCIP15", "PCIP16", "PCIP19", "PCIP22", "PCIP23", "PCIP24", "PCIP25", "PCIP26", "PCIP27", "PCIP29", "PCIP30", "PCIP31", "PCIP38", "PCIP39", "PCIP40", "PCIP41", "PCIP42", "PCIP43", "PCIP44", "PCIP45", "PCIP46", "PCIP47", "PCIP48", "PCIP49", "PCIP50", "PCIP51", "PCIP52", "PCIP54")) 

```

#### _Removal of Demographic Variables_
Additionally, I want to remove proportional race variables from our data set. While interesting for another study this data dose not pertain to our research question at hand, nor do I intuitively see any connection with school demographics and Google search volumes.

The proportion of race is also a variable that would change every year, and given that our search volumes change weekly, and these variables are in a static form it doesn't help give much insight to how search rates change, based on a changing demographic. 

I'm choosing to keep the college "race/ gender flagged" variables as this is likely to be a more constant race indicator less subject to change over time. 

$Note:$ Removing these variables from analysis.   
```{r, echo= FALSE}
# paste(colnames(df), collapse = " + ")
m3 <- feols(md_wk_std_idx ~ UGDS_WHITE + UGDS_BLACK + UGDS_HISP + UGDS_ASIAN + UGDS_AIAN + UGDS_NHPI + UGDS_2MOR + UGDS_NRA + UGDS_UNKN + HBCU + PBI + ANNHI + TRIBAL + AANAPII + HSI + NANTI + MENONLY + WOMENONLY,data = df)
etable(m3)
```
```{r, echo = FALSE}
# paste(dQuote(colnames(df)), collapse = ", ")

#removing variables with alot of na to cut down analysis
df <- df %>% 
  select(-c("UGDS_WHITE", "UGDS_BLACK", "UGDS_HISP", "UGDS_ASIAN", "UGDS_AIAN", "UGDS_NHPI", "UGDS_2MOR", "UGDS_NRA", "UGDS_UNKN")) 
```

#### _Remove High % NULL Columns_
Next, I want to remove any variables that have a high amount of nulls records. While our time series trends data contains many observations, our school features contain less than 2k observations (one per school). Introducing measures that have a high proportion of NULL records can quickly reduce our sample size and add unwanted context to our analysis.

My Null removal criteria is rather arbitrary, however I'm selecting a cut off at 25% Null, meaning that I'm rejecting any variables that have *more than 25% of schools* failing to report on them. 

```{r, echo=FALSE}
x <- map(df, ~mean(is.na(.))) 
y <- data.frame(t(sapply(x,c)))
ty <- as.data.frame(t(y))
ty <- ty %>% 
  filter(V1 >.25)
y = as.data.frame(t(ty))
# paste(dQuote(colnames(y)), collapse = ", ")

```

Just for good measure, I'm going to run a very simply linear model to see if there is any variables worth looking into more. 

$Note:$ Most of these variables are related to (SAT & ACT Scores) which I don't find particularly helpful for this analysis either. There is however information pertaining to "Avg Net School Price" which could be a driving factor in search trends. As a result I've  removed SAT and ATC variables from analysis. 
```{r, echo= FALSE}

# temp df to convert Na to 0
df_m <- df %>%
  mutate_at(c(8:75), ~replace_na(.,0))

m2 <- feols(md_wk_std_idx ~ RELAFFIL + SATVR25 + SATVR75 + SATMT25 + SATMT75 + SATWR25 + SATWR75 + SATVRMID + SATMTMID + SATWRMID + ACTCM25 + ACTCM75 + ACTEN25 + ACTEN75 + ACTMT25 + ACTMT75 + ACTWR25 + ACTWR75 + ACTCMMID + ACTENMID + ACTMTMID + ACTWRMID + SAT_AVG + NPT4_PUB.AVERAGE.ANNUAL.COST + NPT4_PRIV + NPT41_PUB + NPT42_PUB + NPT43_PUB + NPT44_PUB + NPT45_PUB + NPT41_PRIV + NPT42_PRIV + NPT43_PRIV + NPT44_PRIV + NPT45_PRIV + RET_FTL4 + RET_PT4 + RET_PTL4 + C200_L4_POOLED_SUPP ,data = df_m)
etable(m2)
```

From the results (above) I conclude that these variables do not have individual or cumulative explanatory power over our dependent variable (search volume).

***

Looking more closely into the "Avg Net School Price" variables *Pct.Valid* statistic (below) I find that there variables only consist of **~ 30% -60%** viable records, telling me that majority of the data for these variables are NA.

```{r, echo=FALSE}
temp_df <- df %>% 
  select(c(NPT4_PRIV, NPT41_PUB, NPT42_PUB, NPT43_PUB, NPT44_PUB, NPT45_PUB, NPT41_PRIV, NPT42_PRIV, NPT43_PRIV, NPT44_PRIV, NPT45_PRIV))

descr(temp_df,
  headings = FALSE, # remove headings
  stats = "common", # most common descriptive statistics
  transpose = TRUE
)
```
$Note:$ Due to the specificity of the question this is expected. Thus im keeping "school pricing" variables in the data, but *converting* `NULLs` to 0 to reflect the none answer.

***

```{r, echo = FALSE}
#removing variables with a lot of na to cut down analysis
df <- df %>% 
  select(-c("RELAFFIL", "SAT_AVG_ALL", "SATVR25", "SATVR75", "SATMT25", "SATMT75", "SATWR25", "SATWR75", "SATVRMID", "SATMTMID", "SATWRMID", "ACTCM25", "ACTCM75", "ACTEN25", "ACTEN75", "ACTMT25", "ACTMT75", "ACTWR25", "ACTWR75", "ACTCMMID", "ACTENMID", "ACTMTMID", "ACTWRMID", "SAT_AVG", "NPT4_PUB.AVERAGE.ANNUAL.COST", "RET_FTL4", "RET_PT4", "RET_PTL4", "C150_4_POOLED_SUPP.REPORTED.GRAD.RATE")) 

# temp df to convert Na to 0
df <- df %>%
  mutate_at(c(26:36), ~replace_na(.,0))

# colnames(df)
# cat(paste(colnames(df), collapse = "\n "))
```


# Variables of Interest
After variable removal we are left with the following set for analysis. 


|Variable Name|Desc|
|:----|:----|
|schname|school name|
|after_score_card|binary variable indicating if records is after score card release 2015-09-01|
|start_of_week|start of week for reported trend datastart_of_month|start of month for reported trend datamd_wk_std_idx|Normalized Google trends search volumne index|
|month_num|Numeric value for month|
|CITY|City|
|STABBR|State postcode|
|HCM2|Schools that are on Heightened Cash Monitoring 2 by the Department of Education|
|PREDDEG|Predominant degree awarded|
|CONTROL|Control of institution|
|LOCALE|Locale of institution|
|HBCU|Flag for Historically Black College and University|
|PBI|Flag for predominantly black institution|
|ANNHI|Flag for Alaska Native Native Hawaiian serving institution|
|TRIBAL|Flag for tribal college and university|
|AANAPII|Flag for Asian American Native American Pacific Islander-serving institution|
|HSI|Flag for Hispanic-serving institution|
|NANTI|Flag for Native American non-tribal institution|
|MENONLY|Flag for men-only college|
|WOMENONLY|Flag for women-only college|
|SAT_AVG_ALL|Average SAT equivalent score of students admitted for all campuses rolled up to the 6-digit OPE ID|
|DISTANCEONLY|Flag for distance-education-only education|
|UGDS|Enrollment of undergraduate degree-seeking students|
|PPTUG_EF|Share of undergraduate, degree-/certificate-seeking students who are part-time|
|CURROPER|Flag for currently operating institution, 0=closed, 1=operating|
|NPT4_PRIV|Average net price for Title IV institutions (private for-profit and nonprofit institutions)|
|NPT41_PUB|Average net price for $0-$30,000 family income (public institutions)|
|NPT42_PUB|Average net price for $30,001-$48,000 family income (public institutions)|
|NPT43_PUB|Average net price for $48,001-$75,000 family income (public institutions)|
|NPT44_PUB|Average net price for $75,001-$110,000 family income (public institutions)|
|NPT45_PUB|Average net price for $110,000+ family income (public institutions)|
|NPT41_PRIV|Average net price for $0-$30,000 family income (private for-profit and nonprofit institutions)|
|NPT42_PRIV|Average net price for $30,001-$48,000 family income (private for-profit and nonprofit institutions)|
|NPT43_PRIV|Average net price for $48,001-$75,000 family income (private for-profit and nonprofit institutions)|
|NPT44_PRIV|Average net price for $75,001-$110,000 family income (private for-profit and nonprofit institutions)|
|NPT45_PRIV|Average net price for $110,000+ family income (private for-profit and nonprofit institutions)|
|PCTPELL|Percentage of undergraduates who receive a Pell Grant|
|RET_FT4|First-time, full-time student retention rate at four-year institutions|
|PCTFLOAN|Percent of all federal undergraduate students receiving a federal student loan|
|UG25abv|Percentage of undergraduates aged 25 and above|
|GRAD_DEBT_MDN_SUPP|Median debt of completers, suppressed for n=30|
|GRAD_DEBT_MDN10YR_SUPP|Median debt of completers expressed in 10-year monthly payments, suppressed for n=30|
|RPY_3YR_RT_SUPP|3-year repayment rate, suppressed for n=30|
|md_reported_ern_10yr|median reported earning 10 yr|
|gt_25k_p6|Share of students earning over $25,000/year (threshold earnings) 6 years after entry|

***

# Feature Creation

#### _Create Feature - log of earnings_
Our research questions states that we want to look at the *difference between “high-earning” vs “low-earning” colleges*, so we need to define *“high” vs “low”*. 

```{r, echo= FALSE}
ggplot(df, aes(x= md_reported_ern_10yr)) + geom_density()

```

This graph represents the density of *median earnings of graduates ten years after graduation for each college*.
Density in this case is the `count` of colleges that fall in that earning bin.

We can see that this data is skewed to the right, indicating that while most schools "median earning 10 years after graduation" tends to fall right around $40,000. We do have some outlire schools who report very high earnings above 150K.

$Note:$ a high concentration at ~ 55K threshold. Data is self reported and I theorize that 55K is often whats input for any salary range between 50K - 60K. 

***

To adjust for the skew and normalizes earning data I'm going to take the `log` of earnings. 

```{r, echo= FALSE}
ggplot(df, aes(x= log(as.numeric(md_reported_ern_10yr)))) + geom_density()
```

From this graph we can see that data more closely represents a normal curve when represented as a log().
This indicates that if we are to use the earnings in this analysis (Which we are) we should probably explore its in its log() form  representation.

***

```{r,  echo = FALSE, message=FALSE, warning=FALSE}
df <- df %>%
  mutate(log_mean_10_yr_earnings = log(md_reported_ern_10yr))
```
#### _Create Feature - low-med-high_

The distribution of earnings (above) are important as it impacts how we should set *"High" vs "Low"* earning buckets for schools.  We know from the above that will not want to accept the `mean` as a splitting point because the data is skewed by a handful of very high earners. 

To set the *"High" Vs "Low"* cut off point I initially looked at the **Data Dictionary**. 
The Data Dictionary consistently categorizes income & earning at the following levels:

- low-income (less than $30,000)
- middle-income (between $30,000 and $75,000)
- high-income (above $75,000)

Since this is the established threshold I thought about using it as well.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
df <- df %>%
  mutate(low_med_high_ern = case_when(as.numeric(md_reported_ern_10yr) < 30000 ~ 'Low',
                             as.numeric(md_reported_ern_10yr) > 75000 ~ 'High',
                             TRUE ~ 'Med'))
```

However there is a problem. Our research question specifically asks about *"high" vs "low"* earners, and if I was to use this classification of "high" vs "low" I would have to _discard the medium_, which would by design cut out the mean of my sample population and ~ **90%** of my observations. (see below)

```{r, echo = FALSE}
freq(df$low_med_high_ern, report.nas = FALSE, cumul = FALSE)
```
$Note:$ This methodology of determining *High and Low* doesn't fit this analysis.

***

#### _Create Feature - low-high_
To account for this issue I've opted to split *"high" vs "low"* by `median` for the entire sample. Median is a better mark for centrality than mean given skewed data.

```{r, echo= FALSE}
# median earnings
median_val = median(df$md_reported_ern_10yr)
df <- df %>%
  mutate(low_ern_school = case_when(md_reported_ern_10yr <= median_val ~ TRUE,
                             TRUE ~ FALSE))

```
```{r, echo = FALSE}
freq(df$low_ern_school, report.nas = FALSE, cumul = FALSE)
```

$Note:$ This is more fitting way to decide low vs high earnings for this data set. 

***

# Describe Data Set

#### _after_score_card_
This table shows the proportion of records that fall into each bucket.

```{r, echo= FALSE}
freq(df$after_score_card, report.nas = FALSE, cumul = FALSE)
```

$Note:$ Our sample is lopsided (~80% of the records are before the score card)
Typically I would adjust for this using `SMOTE()` under_sampling (However R wont load the SMOTE function at the moment)

***

#### _low-med-high-earn_
This is the frequency of earning buckets as provided by the `data dictionary` threshold. 

```{r, echo= FALSE}
freq(df$low_med_high_ern, report.nas = FALSE, cumul = FALSE)
```

$Note:$ Only data dictionary's method depicts ~2% of the schools as high earners

***

#### _low-high-earn_
(Below) Is the frequency of earning buckets "High" vs "Low" based on the median of earnings across all predominantly bachelors schools.

```{r, echo= FALSE}
freq(df$low_ern_school, report.nas = FALSE, cumul = FALSE)
```

$Note:$ More even split of data and schools fall in either the "high" threshold or in the "low" threshold.

***

#### _Descriptive Stats_
```{r, echo=FALSE}
descr(df,
  headings = FALSE, # remove headings
  stats = "common", # most common descriptive statistics
  transpose = TRUE
)
```
To draw attention to a few interesting metrics:

- Most of our data is populated without NA's records be design
- Median earning range form 16K to 166K with a mean at ~ 50
- we have a university where 98% of its students receive Pell Grant (intesting stat)
- net avg priving ranges from 0 to 45774.00 with a mean of 14079.67

***

# EDA

#### _Time Series_
Time is a variable that could both be related to when the report was release as well as how the search volumes changes across time. 
This means that anything that changes over time is a potential source of endogeneity, even if it isn't inherently related to the Scorecard. 

```{r, echo = FALSE,warning =FALSE}
agg <-  df %>% 
  group_by(low_ern_school, start_of_week) %>% 
  summarise(md_wk_std_idx=mean(md_wk_std_idx)) %>% 
  arrange(., start_of_week)

ggplot(data = agg, mapping = aes(x=start_of_week, y=md_wk_std_idx, color= low_ern_school, group=low_ern_school))+
  geom_path()+
  geom_point()+
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,5), expand = c(0,0))+
  scale_x_date(date_breaks = '6 month', date_labels = '%b %y')+
  labs(x = NULL, y = 'Avg Normalized Index By Week', color = 'Low Earning')+
  theme_bw()
```

In the graph (above) the normalized Google search ratings tend be highly correlated between schools that are considered *"low" vs "high"* earners. 

$Note:$ If you _visually compare_ *"high" vs "Low"* earning schools search volume over time, you see that the dispersion of search index becomes wider at about 2015-09-1, the same time College Score Card Was released.  

***

Looking at this at a more aggregated level by month. 
```{r, echo= FALSE, warning =FALSE}
agg <-  df %>% 
  group_by(low_ern_school, start_of_month) %>% 
  summarise(md_wk_std_idx=mean(md_wk_std_idx)) %>% 
  arrange(., start_of_month)

ggplot(data = agg, mapping = aes(x=start_of_month, y=md_wk_std_idx, color= low_ern_school, group=low_ern_school))+
  geom_path()+
  geom_point()+
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,5), expand = c(0,0))+
  scale_x_date(date_breaks = '3 month', date_labels = '%b %y')+
  labs(x = NULL, y = 'Avg Normalized Index By Month', color = 'Hi or Low Earning')+
  theme_bw()
```

$Note:$ If you _visually compare_ *"high" vs "Low"* earning schools search volume over month, we see the same dispersion as before. 

***

#### _Variance by Time_
If we plot the data graphically to look at the relationship between search volumes and *"log_mean_10_yr_earnings"* we get something that doesn't depict a clear relationship but does shed some insights on variance. 


```{r, echo= FALSE}
agg <-  df %>% 
  group_by(schname, after_score_card, low_ern_school) %>% 
  summarise(md_wk_std_idx=mean(md_wk_std_idx), 
            log_mean_10_yr_earnings = mean(log_mean_10_yr_earnings))

median_line = median(df$log_mean_10_yr_earnings)

ggplot(data=agg, aes(y = md_wk_std_idx, x = log_mean_10_yr_earnings))+
  geom_point(alpha = 0.6, aes(color = factor(after_score_card)))+
  geom_vline(xintercept = median_line)+
  geom_hline(yintercept = 0) +
  labs(y = 'md_wk_std_idx', x = 'Log Median 10 Yr Earnings', color = 'after_score_card')

```

Notice that the variation in Google search volumes after (*Blue*) college score card was released is greater than before (*Red*) it was released.
This gives us a good idea that the launch of College Score Card did change search behavior in some fashion.

***

#### _Avg Net Pricing_
Pricing intuitively plays a big factor in school research, applications, and selections so I wanted to look at this variable a bit more closly. 

(Below) I have plotted the relationship between pricing and earnings. Not a huge surprise here but price and future earnings are positively related. 
 
  
```{r, echo= FALSE}
agg <-  df %>% 
  group_by(schname, after_score_card, low_ern_school) %>% 
  summarise(NPT4_PRIV =mean(NPT4_PRIV),
            log_mean_10_yr_earnings = mean(log_mean_10_yr_earnings)) %>% 
  filter(NPT4_PRIV != 0)

ggplot(data=agg, aes( y = log_mean_10_yr_earnings, x = NPT4_PRIV))+
  geom_point( size=1, alpha = 0.2, aes(color = factor(low_ern_school)))+
  geom_smooth(method=lm, level= .95)+
  labs( y = 'Median 10 Yr Earnings', x = 'NPT4_PRIV', color = 'low_ern_school')


```

Note that there does not appear to be any sort of polynomial shape to our plot, so this does not suggest exploring other non linear models to describe the relationship.

This does, however, suggest that I should be including the price related variables in my models so that I don't add omitted variable bias to my earnings variable.  

***

To test the strength of this relationship I'll regress *log_mean_10_yr_earnings* on price (*NPT4_PRIV*).

Here I'm testing this relationship with `I(NPT4_PRIV/ 1000)` to make the unit measurement of price more understandable. 
```{r, warning=FALSE, message=FALSE}
# Regress earning and price
m1 <- feols(log_mean_10_yr_earnings ~ I(NPT4_PRIV/ 1000), data = agg)

etable(m1, digits = 3)
```

$Note:$ This statisticaly significant coefficient would indicate that for each additional **$1000** spent on school your *log(median earnings)* would increase by a **0.018**  (which is approximately **1.8%** increase in expected earnings). The price of school is in no way correlated to the Score Card being released, however we should add the *NPT4_PRIV* variable to our model as it is obviously correlated to *earnings* which is central to our study. 

***

# Modeling
Question: Did the release of the Scorecard shift student interest to high-earnings colleges relative to low-earnings ones
target variable is interest.  

#### _Basic Linear Model_

Basic linear model to regress search volume with a control for week. 
```{r, warning=FALSE, message=FALSE}
m1 <- feols(md_wk_std_idx ~ log_mean_10_yr_earnings + after_score_card + low_ern_school + NPT4_PRIV + start_of_week, data = df)
etable(m1, digits = 3)
```
This model suggest that we have three statistically significant variables. 2 at the 0.1% level and 1 at 5% level.  

- after_score_cardTRUE == 0.228***
- NPT4_PRIV == 2.98e-7*
- start_of_week == -0.0007***

The Model suggest that after the college score card was release there is a .228 index point increase relative to before the score card. This would imply that the college score card increased traffic. However, this doesn't help answer how it different between *high and low* ranking schools , which is our predominate focus. 


***
Additionally, I was surprised that the *log_mean_10_yr_earnings* wasn't significant.
Using the wald() test to looking more closely at the effect of `log_mean_10_yr_earnings`.

```{r, echo=FALSE}
wald(m1, c('log_mean_10_yr_earnings'))
```
$Note:$ Based on our Wald test we fail to reject the null hypothesis that `log_mean_10_yr_earnings`'s cumulative effect is 0 as the p value = .11. This indicates that `log_mean_10_yr_earnings` is not *by its self* important when explaining the variation in search volumes, however because we know it is correlated to school price variable I'm going to leave it in to avoid omitted variable bias. 

***

#### _Exploring additional variables_

I want to explore some of the other school factors and categorical variables to see if anything else stands out. 

```{r, warning=FALSE, message=FALSE}
m3 <- feols(md_wk_std_idx ~ log_mean_10_yr_earnings +after_score_card + low_ern_school + after_score_card:low_ern_school + NPT4_PRIV + 
            start_of_week + factor(LOCALE) + factor(HBCU) + factor(HCM2) + factor(PBI) + factor(ANNHI) +
            factor(TRIBAL) + factor(AANAPII) + factor(HSI) + factor(NANTI), data = df)

m4 <- feols(md_wk_std_idx ~ log_mean_10_yr_earnings +after_score_card + low_ern_school + after_score_card:low_ern_school + NPT4_PRIV + 
            start_of_week + factor(MENONLY) + factor(WOMENONLY) , data = df)
etable(m3, m4, digits = 3)
```

$Note:$ We don't see anything suggesting an important relationship between that and target variable we are trying to explain. 

***

#### _Adding interaction terms_
Lastly, to dig into to focus of our research questions and explore how interest shifted **after** the release of college score card for  high-earnings colleges **relative** to low-earnings I need to explore the interaction between those variables.

```{r, warning=FALSE, message=FALSE}
m2 <- feols(md_wk_std_idx ~ after_score_card + low_ern_school + low_ern_school:after_score_card + NPT4_PRIV + start_of_week, data = df)
etable(m2, digits = 3)
```

_Interpretation_:
We have an intercept for our index variable at 12.0

Coefficient on "after_score_cardTRUE" by its self:
"after_score_cardTRUE" = 0.188***

- This variable represents the effect of launching the college score card for high earning colleges.
- This means that the effect of the scorecard (when the school is in the upper earnings bucket) on search volumes in is positive **0.188**.
- Meaning that among high earning colleges, Google search volumes **after** the score card released have search volumes of **0.188** points **higher** than they did **before** the college score card was released. 

$Take Away:$ score card being released increased web traffic for high earning schools. 

$Note:$This is statistical significant at the **0.1%** level

***

_Interaction Term_: 
"after_score_cardTRUE x low_ern_schoolTRUE" = **0.080*****

-This is the **difference** in the effect of the scorecard variable **between** *low and high earning schools*
-The effect of releasing the *college score card* on *Google search volumes* is **0.080** points **higher for low earning schools** than it is for **higher earning colleges*.

$Take Away:$ The effect of the score card on *Google search volumes* is *higher for low earning colleges* than it is for **higher earning colleges**.

$Note:$This is statistical significant at the **0.1%** level

***

_The over all effect_:

In this case for the **low earning colleges** (0.188 + 0.080 = 0.268) would imply that the effect of releasing the college score card is **0.268 points higher** than before college score card for **low earning colleges**. 

$Take Away:$ This increases search volumes for the low earning columns

$Note$: Findings are statistical significant when regressed on the the Google search index. 

***

$OverAllEffect:$ When the college score card released it cause and increased in search traffic for both the **low and high earning** colleges, but it increased the search traffic for low earning schools **more** than it did for high earning schools.

$Note$: Findings are statistical significant at the 0.1% level.