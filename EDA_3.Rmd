---
title: "EDA"
author: "Andrew Zinkan"
date: "8/3/2022"
output: html_document
---

# TODO:
Read the instructions again!!!!!!!

- double check the index standardizetion. might need to group by first. 
- need to move into modelign ASAP. 
- open up variables on the df and do backward feature emilination to select. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, ech0 = FALSE, results= "hide", message=FALSE, warning=FALSE}
# Load initial packages
# install.packages("rlist")
library(tidyverse)
library(ggplot2)
library(dplyr)
library(vtable)
library(fixest)
library(Hmisc)
# library(timetk)
# library(DMwR)
library(tidyr)
library(rlist)
```
###  Restated Question:

The College Scorecard was released at the start of September 2015. Among colleges that predominantly grant bachelor’s degrees, did the release of the Scorecard shift student interest to high-earnings colleges relative to low-earnings ones (as proxied by Google searches for keywords associated with those colleges)?

#### _Load Data Transfromation Files_
```{r, results= "hide", message=FALSE, warning=FALSE}
google_trends_master <-  read.csv("Generated_data/google_trends_master.csv", na.strings = c("","NA","NULL",NULL))
school_master <- read.csv("Generated_data/school_master.csv", na.strings = c("","NA","NULL",NULL))

```

```{r, echo = FALSE ,results= "hide", message=FALSE, warning=FALSE}
google_trends_master <- google_trends_master %>% 
  mutate(start_of_week = as.Date(start_of_week, format="%Y-%m-%d"))
  
```

#### _Join the final data for analysis_
```{r}
# Make the final data set and provide reasoning
df <- google_trends_master %>% 
  inner_join(school_master, by = c("opeid", "unitid"))

df <- df %>% 
  rename(schname = schname.y) %>% 
  select(-c("schname.x", "unitid", "opeid", "opeid6", "INSTNM", "INSTURL", 
            "NPCURL"))
  

```

FOOT NOTES: 
The first thing i want to do becase we have some many variables is reducde the columns to whats likly important. 

#### _Remove High % NULL Columns_
because there are a lot of variables in this file im going to reduce our viable column list quickly by eliminating measures with a lot of Null entries.

This is rather arbitrary, however im selecting cut off of 25% null values and rejecting anything higher for the time being. Meaning that for consideration im only allowing up to 25% of the records for column to be null. 

If over 25% of the records are null for a column im discarding them for now. 

Using this code chunk to create my column list, Which i will then hide from the school data set. 
```{}
x <- map(df, ~mean(is.na(.))) 
y <- data.frame(t(sapply(x,c)))
ty <- t(y)
mylist <- list()

for(i in 1:123)
 {
  if (ty[i] >.25) {
  mylist <- append(mylist, row.names(ty)[i])
  }
  else {
  }
}
y <- data.frame(t(sapply(mylist,c)))
ty <- t(y)
<!-- paste(ty, collapse = '\n') %>% cat() -->

```

```{r, echo = FALSE}
#removing variables with alot of na to cut down analysis
df <- df %>% 
  select(-c("RELAFFIL",
            "SATVR25",
            "SATVR75",
            "SATMT25",
            "SATMT75",
            "SATWR25",
            "SATWR75",
            "SATVRMID",
            "SATMTMID",
            "SATWRMID",
            "ACTCM25",
            "ACTCM75",
            "ACTEN25",
            "ACTEN75",
            "ACTMT25",
            "ACTMT75",
            "ACTWR25",
            "ACTWR75",
            "ACTCMMID",
            "ACTENMID",
            "ACTMTMID",
            "ACTWRMID",
            "SAT_AVG",
            "SAT_AVG_ALL",
            "NPT4_PUB.AVERAGE.ANNUAL.COST",
            "NPT4_PRIV",
            "NPT41_PUB",
            "NPT42_PUB",
            "NPT43_PUB",
            "NPT44_PUB",
            "NPT45_PUB",
            "NPT41_PRIV",
            "NPT42_PRIV",
            "NPT43_PRIV",
            "NPT44_PRIV",
            "NPT45_PRIV",
            "RET_FTL4",
            "RET_PT4",
            "RET_PTL4",
            "C200_L4_POOLED_SUPP")) 

```

```{r, echo=FALSE}
var_df <- df %>% 
  top_n(10)

# write.csv(var_df, "C:\\Users\\andre\\OneDrive\\R_Projects\\Economtrics-5300\\Generated_data\\df.csv", row.names=FALSE)
```
# TODO:
- reset the descriptive prints with correct index discribe() func

#### _Variables of interest_

- variables identified to be related to our reaserch question. 


|Variable|Description|
|:----|:----|
|start_of_week|the start of the week for the index value refers to|
|after_score_card|Is week after the score card date 2015-09-01. T/F|
|md_wk_std_idx|Normalized Index by school term|
|schname|school name|
|CITY|City|
|STABBR|State postcode|
|HCM2|Schools that are on Heightened Cash Monitoring 2 by the Department of Education|
|PREDDEG|Predominant degree|
|CONTROL|Control of institution|
|LOCALE|Locale of institution|
|HBCU|Flag for Historically Black College and University|
|PBI|Flag for predominantly black institution|
|ANNHI|Flag for Alaska Native Native Hawaiian serving institution|
|TRIBAL|Flag for tribal college and university|
|AANAPII|Flag for Asian American Native American Pacific Islander-serving institution|
|HSI|Flag for Hispanic-serving institution|
|NANTI|Flag for Native American non-tribal institution|
|MENONLY|Flag for men-only college|
|WOMENONLY|Flag for women-only college|
|PCIP01|Percentage of degrees awarded in Agriculture, Agriculture Operations, And Related Sciences.|
|PCIP03|Percentage of degrees awarded in Natural Resources And Conservation.|
|PCIP04|Percentage of degrees awarded in Architecture And Related Services.|
|PCIP05|Percentage of degrees awarded in Area, Ethnic, Cultural, Gender, And Group Studies.|
|PCIP09|Percentage of degrees awarded in Communication, Journalism, And Related Programs.|
|PCIP10|Percentage of degrees awarded in Communications Technologies/Technicians And Support Services.|
|PCIP11|Percentage of degrees awarded in Computer And Information Sciences And Support Services.|
|PCIP12|Percentage of degrees awarded in Personal And Culinary Services.|
|PCIP13|Percentage of degrees awarded in Education.|
|PCIP14|Percentage of degrees awarded in Engineering.|
|PCIP15|Percentage of degrees awarded in Engineering Technologies And Engineering-Related Fields.|
|PCIP16|Percentage of degrees awarded in Foreign Languages, Literatures, And Linguistics.|
|PCIP19|Percentage of degrees awarded in Family And Consumer Sciences/Human Sciences.|
|PCIP22|Percentage of degrees awarded in Legal Professions And Studies.|
|PCIP23|Percentage of degrees awarded in English Language And Literature/Letters.|
|PCIP24|Percentage of degrees awarded in Liberal Arts And Sciences, General Studies And Humanities.|
|PCIP25|Percentage of degrees awarded in Library Science.|
|PCIP26|Percentage of degrees awarded in Biological And Biomedical Sciences.|
|PCIP27|Percentage of degrees awarded in Mathematics And Statistics.|
|PCIP29|Percentage of degrees awarded in Military Technologies And Applied Sciences.|
|PCIP30|Percentage of degrees awarded in Multi/Interdisciplinary Studies.|
|PCIP31|Percentage of degrees awarded in Parks, Recreation, Leisure, And Fitness Studies.|
|PCIP38|Percentage of degrees awarded in Philosophy And Religious Studies.|
|PCIP39|Percentage of degrees awarded in Theology And Religious Vocations.|
|PCIP40|Percentage of degrees awarded in Physical Sciences.|
|PCIP41|Percentage of degrees awarded in Science Technologies/Technicians.|
|PCIP42|Percentage of degrees awarded in Psychology.|
|PCIP43|Percentage of degrees awarded in Homeland Security, Law Enforcement, Firefighting And Related Protective Services.|
|PCIP44|Percentage of degrees awarded in Public Administration And Social Service Professions.|
|PCIP45|Percentage of degrees awarded in Social Sciences.|
|PCIP46|Percentage of degrees awarded in Construction Trades.|
|PCIP47|Percentage of degrees awarded in Mechanic And Repair Technologies/Technicians.|
|PCIP48|Percentage of degrees awarded in Precision Production.|
|PCIP49|Percentage of degrees awarded in Transportation And Materials Moving.|
|PCIP50|Percentage of degrees awarded in Visual And Performing Arts.|
|PCIP51|Percentage of degrees awarded in Health Professions And Related Programs.|
|PCIP52|Percentage of degrees awarded in Business, Management, Marketing, And Related Support Services.|
|PCIP54|Percentage of degrees awarded in History.|
|DISTANCEONLY|Flag for distance-education-only education|
|UGDS|Enrollment of undergraduate degree-seeking students|
|UGDS_WHITE|Total share of enrollment of undergraduate degree-seeking students who are white|
|UGDS_BLACK|Total share of enrollment of undergraduate degree-seeking students who are black|
|UGDS_HISP|Total share of enrollment of undergraduate degree-seeking students who are Hispanic|
|UGDS_ASIAN|Total share of enrollment of undergraduate degree-seeking students who are Asian|
|UGDS_AIAN|Total share of enrollment of undergraduate degree-seeking students who are American Indian/Alaska Native|
|UGDS_NHPI|Total share of enrollment of undergraduate degree-seeking students who are Native Hawaiian/Pacific Islander|
|UGDS_2MOR|Total share of enrollment of undergraduate degree-seeking students who are two or more races|
|UGDS_NRA|Total share of enrollment of undergraduate degree-seeking students who are non-resident aliens|
|UGDS_UNKN|Total share of enrollment of undergraduate degree-seeking students whose race is unknown|
|PPTUG_EF|Share of undergraduate, degree-/certificate-seeking students who are part-time|
|CURROPER|Flag for currently operating institution, 0=closed, 1=operating|
|PCTPELL|Percentage of undergraduates who receive a Pell Grant|
|RET_FT4|First-time, full-time student retention rate at four-year institutions|
|PCTFLOAN|Percent of all federal undergraduate students receiving a federal student loan|
|UG25abv|Percentage of undergraduates aged 25 and above|
|GRAD_DEBT_MDN_SUPP|Median debt of completers, suppressed for n=30|
|GRAD_DEBT_MDN10YR_SUPP|Median debt of completers expressed in 10-year monthly payments, suppressed for n=30|
|RPY_3YR_RT_SUPP|3-year repayment rate, suppressed for n=30|
|C150_4_POOLED_SUPP.REPORTED.GRAD.RATE|#N/A|
|md_reported_ern_10yr|Median earnings of students working and not enrolled 10 years after entry|
|gt_25k_p6|Share of students earning over $25,000/year (threshold earnings) 6 years after entry|


# Describe Data Set
Data will be descibed for selected variables on interest
Lets start by looking at some basic statistics for each data source. 
```{r}
di <- describe(df)
```
Observations:
1386 unique university observations
Total of 733823 records (We have many entried per induvidual) -> Pannel Data
No N/A values
```{r}
# time frame data
print(di[5])
```
Observations:
No N/A Data
157 Unique Weeks accounted for in the data set
ranging from 2013-03-31 -> 2016-03-27
  

```{r}
# index info
print(di[6:8])
```
Observations:
No N/A Data
Mean raw index rating = 48.11
Index ranging from 1 -> 100

**I'll call out a few additional observations of interest here but i wont be goign as in depth in later annotations. 

Gini's mean difference (Gmd) is a measure of Variability fairly low as there is a set range of indexing.
Can also tell from the Continuose dendist functions (CDF) numbers that the data is pretty normaly distributed and that 50% of its observations are under the value of 48 (which makes perfect sense as the mean is also 48)



```{r}
# normalized index info
print(di[11])
```
Observations:
No N/A Data
Mean standardized rating from ~ -5 -> ~13. 
This is a normalized index rating, has not direct interpretation. 
```{r}
# normalized index info
print(di[9])
```
Observations:
No N/A Data
Mean raw index rating = 48.11

**Note that we have lopsided data (only 19% of our records are taken after the score card came out)
we'll want to adjust our sample to be random and more even distribution of true and false records. 

#TODO
- see if you can make this happen but dont try to  hard.
dont mess with SMOTE becase it could mess up r and you dont have time

#### _selecting random sample_

```{r}
# Before
# table(google_trends_master$after_score_card)
# under_sample <- SMOTE(after_score_card ~ ., google_trends_master, perc.over = 600,perc.under=100)
# table(under_sample$after_score_card)

```











# Feature creation

#### _Identify "high-earning" and "low-earning" Earning Schools_
Our research questions states “high-earning” vs “low-earning” colleges earning, so how can we define “high” vs “low”? Lets first look at the distribution of earnings among colleges.
```{r}
# Histogram overlaid with kernel density curve
ggplot(df, aes(x=md_reported_ern_10yr)) +
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   binwidth= 10,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666", stat="count")  # Overlay with transparent density plot
```

This graph represents the density of "median earnings of graduates ten years after graduation for each college".
Density in this case is the `count` of colleges that fall in that earning spectrum.

This plot has a lot of noise in it though so lets smooth it out and look at densisty. 
```{r}
ggplot(df, aes(x= md_reported_ern_10yr)) + geom_density()

```

Now that the data is smoothed we get a better perspective of the distribution of earnings. 
We can see tho that this data is skewed to the right, indicating that while most schools median earning 10 years after graduation tends to fall right around 40,000 we have select schools who report very high earnings above 150,000. 

to adjust for the skew and normalizes the data a bit we'll take the log of earnings
```{r}
ggplot(df, aes(x= log(as.numeric(md_reported_ern_10yr)))) + geom_density()
```
Now we can see that we account for the long tails and tthat the data more closely represents a normal curve when represented as a log().
This indicate that if we are to use the earnings in this analysis (Which we are) we should probably explore its representation inits log() form. 

#### _Create Feature - log of earnings_
Storing the log value as a column. 
```{r,  message=FALSE, warning=FALSE}
df <- df %>%
  mutate(log_mean_10_yr_earnings = log(md_reported_ern_10yr))
```

This also tells us that when setting the "High" vs "Low" Earning schools we will not want to accept the mean as a splitting point as very high or very low earning schools have the ability to skew the mean value

The first way i would attempt to set the "High" Vs "Low" cut off point is by looking at the data dictionary. 
Looking at the data dictionary income/ earning levels are consistently categorized as the following:

- low-income (less than $30,000)
- middle-income (between $30,000 and $75,000)
- high-income (above $75,000)

Since this is the standard according to the data dictionary i dont think it would be wrong to follow suit in our classification.
SO we'll add this classification as a feature for later use.

#### _Create Feature - low-med-high_
```{r, message=FALSE, warning=FALSE}
df <- df %>%
  mutate(low_med_high_ern = case_when(as.numeric(md_reported_ern_10yr) < 30000 ~ 'Low',
                             as.numeric(md_reported_ern_10yr) > 75000 ~ 'High',
                             TRUE ~ 'Med'))
```

Using this classification of high vs low however cuts out the mean of my sample population though and a majority of my observations so ill want to address this differently.  

For this ill be splitting it on the `median` for the entire sample. Selecting not to use mean here as earnings as we saw graphically (above) are highly skewed, median however will help us get a less skewed center. 

#### _Create Feature - low-high_
```{r}
# median earnings
median_val = median(df$md_reported_ern_10yr)
df <- df %>%
  mutate(low_high_ern = case_when(md_reported_ern_10yr <= median_val ~ 'Low',
                             TRUE ~ 'High'))

```

#TODO:
- add month feature
- change week over week as a feature

```{r}
# then add features, interactions, strip month, look for difference or change in metrics each week 
# This section we'll identify high earning school/ how and why, etc.  
# TODO: use ymd() on date. see https://nickch-k.github.io/EconometricsSlides/Week_06/Week_06_Bonus_Project_Data_Cleaning.html#6
```








# EDA

TODO: DO the below by month too
```{r, warning =FALSE}
agg <-  df %>% 
  group_by(low_high_ern, start_of_week) %>% 
  summarise(md_wk_std_idx=mean(md_wk_std_idx)) %>% 
  arrange(., start_of_week)

ggplot(data = agg, mapping = aes(x=start_of_week, y=md_wk_std_idx, color= low_high_ern, group=low_high_ern))+
  geom_path()+
  geom_point()+
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,5), expand = c(0,0))+
  scale_x_date(date_breaks = '6 month', date_labels = '%b %d %Y')+
  labs(x = NULL, y = 'Avg Normalized Index By Week', color = 'Hi or Low Earning')+
  theme_bw()
```
When looking at the search index ratings schools look to be highly correlated.
Note we do not have continuous data there are gaps in time not accounted for. (WOuld idealy like to inerpolate the values) 

```{r}
agg <-  df %>% 
  group_by(schname, after_score_card, low_high_ern) %>% 
  summarise(md_wk_std_idx=mean(md_wk_std_idx), 
            log_mean_10_yr_earnings = mean(log_mean_10_yr_earnings),
            PCIP04 = mean(PCIP04))

ggplot(data=agg, aes(y = md_wk_std_idx, x = log_mean_10_yr_earnings))+
  geom_point(aes(color = factor(after_score_card)))+
  labs(y = 'Index', x = 'Median 10 Yr Earnings', color = 'after_score_card')

```

```{r}
agg <-  df %>% 
  group_by(schname, after_score_card, low_high_ern) %>% 
  summarise(md_wk_std_idx=mean(md_wk_std_idx), 
            log_mean_10_yr_earnings = mean(log_mean_10_yr_earnings),
            PCIP04 = mean(PCIP04))

ggplot(data=agg, aes(y = md_wk_std_idx, x = log_mean_10_yr_earnings))+
  geom_point(aes(color = factor(low_high_ern)))+
  labs(y = 'Index', x = 'Median 10 Yr Earnings', color = 'low_high_ern')

```


#### _dagity.net for possible factors_


#### _modeling_
```{r}
# start modeling (use what you have learned in class)
# plots and scatter plots
```


```{r}
# library(corrplot)
# 
# write.csv(df, "C:\\Users\\andre\\OneDrive\\R_Projects\\Economtrics-5300\\Generated_data\\master_df.csv", row.names=FALSE)
# 
# 
# df_trim <- df %>% 
#   select(md_wk_std_idx, log_mean_10_yr_earnings,md_reported_ern_10yr, LOCALE, MENONLY,WOMENONLY,UGDS, low_med_high_ern, low_high_ern )
# 
# # Backward feature elimination
# fitall <- lm( md_wk_std_idx ~ .-opeid-unitid, data = df_trim)
# summary(fitall)
# # Pick the main variables
# acme_var = df_trim %>% select(profit_acre, region, Unique_eqt_cnt, area,
#                                 crop_acre, Education_Attainment,
#                                 b_Health_conditions_in_last_two_weeks, nh_num_people)
# summary(acme_var)

```



<!-- library(corrplot) -->

<!-- # Backward feature elimination -->
<!-- fitall <- lm(profit_acre ~ .-clust-nh, data = master_df) -->
<!-- summary(fitall) -->
<!-- # Pick the main variables -->
<!-- acme_var = master_df %>% select(profit_acre, region, Unique_eqt_cnt, area, -->
<!--                                 crop_acre, Education_Attainment, -->
<!--                                 b_Health_conditions_in_last_two_weeks, nh_num_people) -->
<!-- summary(acme_var) -->
<!-- # Get correlations -->
<!-- c <- cor(acme_var %>% select(!c("profit_acre"))) -->
<!-- corrplot(c, method="circle") -->
<!-- # Regress profit by acre on the main acme set -->
<!-- main <- lm(profit_acre ~ region + Unique_eqt_cnt + area + crop_acre +  -->
<!--              Education_Attainment + b_Health_conditions_in_last_two_weeks +  -->
<!--              nh_num_people, data=acme_var) -->
<!-- summary(main) -->
<!-- # Plot model diagnostics starting with the standardized residuals. -->
<!-- hist(rstandard(main), xlab="Standardized Residuals") -->
<!-- # Now plot the residuals vs fitted. -->
<!-- plot(fitted(main), resid(main), xlab="Fitted", ylab="Residuals") -->


<!-- # Regress profit by acre on all with factor(region) and education^2 -->
<!-- edu <- lm(profit_acre ~ factor(region) + Unique_eqt_cnt + area + crop_acre + nh_num_people + -->
<!--             I(Education_Attainment^2) + Education_Attainment + b_Health_conditions_in_last_two_weeks, data=acme_var) -->
<!-- summary(edu) -->
<!-- # Plot model diagnostics starting with the standardized residuals. -->
<!-- hist(rstandard(edu), xlab="Standardized Residuals") -->
<!-- # Now plot the residuals vs fitted. -->
<!-- plot(fitted(edu), resid(edu), xlab="Fitted", ylab="Residuals") -->
