---
title: "Data Transformation"
author: "Andrew Zinkan"
date: "8/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results= "hide", message=FALSE, warning=FALSE}
# Load initial packages
library(tidyverse)
library(ggplot2)
library(dplyr)
library(vtable)
library(fixest)
library(tidyr)
library(lubridate)
```
## Data Transformation for Score Card Data

```{r, echo = FALSE, results= "hide", message=FALSE, warning=FALSE}
# Running the raw data
# Striping the Nulls to NA Values
DataDictRaw <-  read.csv("Data_Exploration/CollegeScorecardDataDictionary.csv", na.strings = c("","NA","NULL",NULL))
IDNameLinkRaw <- read.csv("Data_Exploration/id_name_link.csv", na.strings = c("","NA","NULL",NULL))
ScoreCardDataRaw <- read.csv("Data_Exploration/Most_Recent_Cohorts_Scorecard_Elements.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToFinishRaw <- read.csv("Data_Exploration/trends_up_to_finish.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_1_Raw <- read.csv("Data_Exploration/trends_up_to_inter_1.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_2_Raw <- read.csv("Data_Exploration/trends_up_to_inter_2.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_3_Raw <- read.csv("Data_Exploration/trends_up_to_inter_3.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_4_Raw <- read.csv("Data_Exploration/trends_up_to_inter_4.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_5_Raw <- read.csv("Data_Exploration/trends_up_to_inter_5.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToInter_6_Raw <- read.csv("Data_Exploration/trends_up_to_inter_6.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUmRaw <- read.csv("Data_Exploration/trends_up_to_UM.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUphoenixRaw <- read.csv("Data_Exploration/trends_up_to_UPhoenix.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUtRaw <- read.csv("Data_Exploration/trends_up_to_UT.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToUtmbRaw <- read.csv("Data_Exploration/trends_up_to_UTMB.csv", na.strings = c("","NA","NULL",NULL))
TrendsUpToYorktowneRaw <- read.csv("Data_Exploration/trends_up_to_Yorktowne.csv", na.strings = c("","NA","NULL",NULL))
```

#### _Drop opeid's that are not unique observations_
This is in an effort to remove any `many to one` relationships when joining OPEID
```{r}
Unique_opeid <- ScoreCardDataRaw %>% 
  select(OPEID) %>% 
  group_by(OPEID) %>% 
  summarise(count_rec = n())%>% 
  filter(count_rec == 1) %>% 
  arrange(., desc(count_rec))


# create filter for unique ids
Unique_opeid_filter <- Unique_opeid$OPEID

# filter raw set by the opeids that have unique names
ScoreCardDataStage <- filter(ScoreCardDataRaw, OPEID %in% Unique_opeid_filter)

```
7804 records -> 7738 records


#### _Remove "PrivacySuppressed" earnings data_
This data is directly related to college earnings (our dependent variable), thus we will want to use only known values to regress on. 
Null values in this field don't shed any light on how different independent variables impact an unknown depended variable. 
```{r}
# Remove "PrivacySuppressed" data 
NotSuppressed <- ScoreCardDataStage %>% 
  select(OPEID, md_earn_wne_p10.REPORTED.EARNINGS) %>% 
  filter(md_earn_wne_p10.REPORTED.EARNINGS != c("PrivacySuppressed"))

# create filter for unique ids
NotSuppressed_filter <- NotSuppressed$OPEID

# Remove Suppressed Earnings Data
ScoreCardDataStage <- filter(ScoreCardDataStage, OPEID %in% NotSuppressed_filter)
```
7738 records -> 5584 records
2154 records removed

#### _Drop NULL values in earnings data_
Want to drop null values for the same reason we want to drop the suppressed data 

```{r}
NotNull <- ScoreCardDataStage %>% 
  select(OPEID, md_earn_wne_p10.REPORTED.EARNINGS) %>% 
  filter(!is.na(md_earn_wne_p10.REPORTED.EARNINGS))

# create filter for unique ids
NotNull_Filter <- NotNull$OPEID

# Remove Suppressed Earnings Data
ScoreCardDataStage <- filter(ScoreCardDataStage, OPEID %in% NotNull_Filter)
```
5584 records -> 5584 records

#### _Renaming_
```{r}
ScoreCardDataStage <- ScoreCardDataStage %>% 
  mutate(md_earn_wne_p10.REPORTED.EARNINGS = as.numeric(ScoreCardDataStage$md_earn_wne_p10.REPORTED.EARNINGS))

ScoreCardDataStage <- ScoreCardDataStage %>%
  rename(opeid = OPEID,  
         unitid = UNITID, 
         md_reported_ern_10yr = md_earn_wne_p10.REPORTED.EARNINGS)

```
#TODO:
filter for only operational universities
CURROPER 0=closed, 1=operating

non operational in not a standard when we think of our reaserch question. we dotn ever care about bacheler programs that no longer operate. these tpes of shoools represent them selvs as there own class in my opinion. 


## Data Transformation For School Link Data

#### _Drop non unique opeid's_
Identify opeid's with more than one unique school name
many to 1 relationship with opeid (want to cut that out)

```{r}
Unique_schools <- IDNameLinkRaw %>% 
  select(opeid, schname) %>% 
  group_by(opeid) %>% 
  summarise(count_unique = n_distinct(schname))%>% 
  arrange(., desc(count_unique)) %>% 
  filter(count_unique == 1)

# create filter for unique ids
school_filter <- Unique_schools$opeid

# filter raw set by the opeids that have unique names
IDNameLinkStage <- filter(IDNameLinkRaw, opeid %in% school_filter)
```
Starting with 3595 Schools
Our first filter results in 3557 remaining schools
38 universities removed

#### _Drop non unique Universities Names_
```{r}
# Identify names that have more than one opeid
unique_ids <- IDNameLinkRaw %>% 
  select(opeid, schname) %>% 
  group_by(schname) %>% 
  summarise(count_unique = n_distinct(opeid))%>% 
  arrange(., desc(count_unique)) %>% 
  filter(count_unique < 2)

# create filter for unique ids
school_filter <- unique_ids$schname

# apply another filter to staged set by filtering the opeids that have unique names
IDNameLinkStage <- filter(IDNameLinkStage, schname %in% school_filter)
```
This filter results in 3486 records, removing an additional 71 universities. 


#### _Filter bachelor degree colleges_
Selecting Bachelors Primary Schools & Bachelors Only Schools
Bachelors Only Schools are by default Bachelors Primary
```{r}
# Selecting Bachelors Primary Schools & Bachelors Only Schools
# Bachelors Only Schools are by default Bachelors Primary
Bachelors_primary <- ScoreCardDataStage %>% 
  select(opeid, PREDDEG) %>% 
  filter(PREDDEG >= 3)

# create filter for unique ids
bach_filter <- Bachelors_primary$opeid

# Remove non bachelors from our master id list
IDNameLinkStage <- filter(IDNameLinkStage, opeid %in% bach_filter)
```
This filter results in 1999 records, removing an additional 1487 universities.

#### _Drop "PrivacySuppressed" earnings_
This data is directly related to college earnings (our dependent variable), thus we will want to use only known values to regress on. 
Null values in this field dont shed any light on how different independent variables impact an unknown depended variable. 
```{r}
# Remove "PrivacySuppressed" data 
NotSuppressed <- ScoreCardDataStage %>% 
  select(opeid, md_reported_ern_10yr) %>% 
  filter(md_reported_ern_10yr != c("PrivacySuppressed"))

# create filter for unique ids
NotSuppressed_filter <- NotSuppressed$opeid

# Remove Suppressed Earnings Data
IDNameLinkStage <- filter(IDNameLinkStage, opeid %in% NotSuppressed_filter)
```
This filter results in 1804 records, removing an additional 195 universities.

#### _Drop Null earnings_
Want to drop null values for the same reason we want to drop the suppressed data 

```{r}
NotNull <- ScoreCardDataStage %>% 
  select(opeid, md_reported_ern_10yr) %>% 
  filter(!is.na(md_reported_ern_10yr))

# create filter for unique ids
NotNull_Filter <- NotNull$opeid

# Remove Suppressed Earnings Data
IDNameLinkStage <- filter(IDNameLinkStage, opeid %in% NotNull_Filter)
```
This filter results in 1804 records, removing an additional 0 universities.

## Data Transformation Trend data


#### _Union All Trend Data_
union all the Trends Data as one dataframe
```{r, results= "hide", message=FALSE, warning=FALSE}
# Union the Trends Data
TrendsAppendedDf <- rbind(TrendsUpToInter_1_Raw, TrendsUpToInter_2_Raw, TrendsUpToInter_3_Raw, TrendsUpToInter_4_Raw, TrendsUpToInter_5_Raw, TrendsUpToInter_6_Raw, TrendsUpToUmRaw, TrendsUpToUphoenixRaw, TrendsUpToUtmbRaw, TrendsUpToUtmbRaw, TrendsUpToYorktowneRaw, TrendsUpToFinishRaw )
```

#### _Drop Duplicates_
because this is API generated trends data that has breaks when to many requests are generateed its very possible we have duplicate results. schid as it provided no meaning.
```{r}
# Select remove the schid field (non unique) and no relationship
Trends <- TrendsAppendedDf %>% 
  select( -c("schid"))

Trends <- Trends %>% 
  distinct()
```
This does in fact remove many duplicate records (30894)

#### _Strip Dates_
Score card was added beginning of sept 2015
```{r, message=FALSE, warning=FALSE}
# separate dates
Trends <- separate(Trends, col=monthorweek, into=c('start_of_week', 'end_of_week'), sep=' - ')

# Convert to date type
Trends <- Trends %>% 
  mutate(start_of_week = as.Date(start_of_week, format="%Y-%m-%d")) %>% 
  mutate(end_of_week = as.Date(end_of_week, format="%Y-%m-%d"))

# Before and after scorecard indicator
Trends <- Trends %>% 
  mutate(after_score_card = if_else(start_of_week >= as.Date("2015-09-01", format="%Y-%m-%d"), TRUE, FALSE))

```

#### _No Null Dates_
As this question is related to the function of time we need to deal with the records that are not associated with a time frame.
It should be noted that these records also lack any index information. Likely was complied from source incorrectly or reflects API querries where data was not available at that time.  
```{r}
Trends <- Trends %>% 
  filter(!is.na(start_of_week))
```

#### _Make Indices reasonably comparable_
```{r, message=FALSE, warning=FALSE}
# Key Level Mean
key_level_idx <- Trends %>% 
  select(schname,keyword,index, ) %>% 
  group_by(schname,keyword) %>% 
  summarise(mean_index = mean(index), 
            std_index = sd(index))

# Join raw and idk level
Trends <- Trends %>% 
  inner_join(key_level_idx, by = c("schname", "keyword")) %>% 
  mutate(standard_idx = (index-mean_index)/std_index) %>% 
  select(schname, keyword, start_of_week,  standard_idx, after_score_card)

```

#### _Add Month info_
```{r}
# add month
Trends <- Trends %>% 
  mutate(start_of_month = format(as.Date(start_of_week), "%Y-%m-1"))
```

#### _Aggregate the Trends Data_
grouping by school-week 
Why?:
we need to keep college to tie back what colleges and associated measues imapct searches, we'll group by week as the index is recorded is weekly intervals (keep more data granularity)
```{r}
Trends <- Trends %>% 
  group_by(schname, after_score_card, start_of_week, start_of_month) %>%
  summarise(md_wk_std_idx = median(standard_idx)) %>%
  arrange(., start_of_week )
```
## Master Data


#### _School ID & Metrics_
Using inner join to ensure that we have a complete data set in terms of unitid and opeid that match across or school key and our school data points.
```{r}
school_master <- IDNameLinkStage %>% 
  inner_join(ScoreCardDataStage, by = c("opeid", "unitid"))
```

our final data set for analysis is now comprised of a total of 1798 records by 123 variables. 
```{r}
dim(school_master)
```

#### _Google Trends & School ID_
Using inner join to ensure that we have a only matching school names across our school id info and our google trends info. 
```{r}
google_trends_master <- IDNameLinkStage %>% 
  inner_join(Trends, by = c("schname"))
```


## CSV Out Put
Holding out our mutation steps in this code and producing one school data frame and one goolg etrends data set allows for faster and cleaner downstream modeling. 
```{r}
write.csv(school_master, "C:\\Users\\andre\\OneDrive\\R_Projects\\Economtrics-5300\\Generated_data\\school_master.csv", row.names=FALSE)
```

```{r}
write.csv(google_trends_master, "C:\\Users\\andre\\OneDrive\\R_Projects\\Economtrics-5300\\Generated_data\\google_trends_master.csv", row.names=FALSE)
```



